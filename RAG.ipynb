{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.text import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=10)\n",
    "loader = TextLoader(\"chathistory.txt\") # your text file path (Context)\n",
    "documents = loader.load_and_split(splitter)\n",
    "\n",
    "for doc in documents:\n",
    "    print(doc.page_content)\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\") #one of the embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Chroma to store text embeddings in a vector database.\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Create an instance of Chroma to store text embeddings in a vector database.\n",
    "\n",
    "vector_store = Chroma.from_texts(\n",
    "    texts=chunks,  # The list of text chunks to be embedded and stored.\n",
    "    embedding=embeddings_model,  # The model used for generating embeddings from the text chunks.\n",
    "    collection_name=\"example_collection\",  # Name of the collection where data will be stored in the database.\n",
    "    persist_directory=\"./chroma_langchain_db\"  # Directory path where the vector store data will be saved locally. Remove if persistence is not needed.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternatively Storing text embeddings in InMemoryVectorStore \n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "vector_store = InMemoryVectorStore.from_documents(documents,embeddings)\n",
    "vector_store.similarity_search(\"hey\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing and Generating Dynamic Responses\n",
    "Implementing the conversation flow logic:\n",
    "- Takes a user question about a dating scenario\n",
    "- Updates chat history with the new question\n",
    "- Performs semantic search to find relevant context\n",
    "- Generates a response using our specialized prompt\n",
    "- Updates chat history with the AI's response\n",
    "- Displays the crafted message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"she said she loves taha husssain only?\"\n",
    "chathistory.append(question)\n",
    "docs = [doc.page_content for doc in vector_store.similarity_search(question)]\n",
    "ans = llm.invoke(rizz_prompt.format(chat_history=chathistory,context=docs, user_input=question)).content\n",
    "chathistory.append(ans)\n",
    "print(ans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
